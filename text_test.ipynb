{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      KANYE 2024 https://t.co/Zm2pKcn12t\n",
       "1                      I VOTED üá∫üá∏ https://t.co/hlgIJUST4x\n",
       "2                    KANYE2020 üá∫üá∏ https://t.co/3kd8vrrHZQ\n",
       "3                               üïä https://t.co/tFqpKyQzkY\n",
       "4       The first vote of my life         We are here ...\n",
       "                              ...                        \n",
       "1662    try to avoid any contractual situation where y...\n",
       "1663    You have to protect your ability to create at ...\n",
       "1664    As a creative your ideas are your strongest fo...\n",
       "1665    often people working with the existing conscio...\n",
       "1666    Some people have to work within the existing c...\n",
       "Name: tweet, Length: 1667, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ye-tweets.csv')\n",
    "df = df[\"tweet\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant variable\n",
    "PUNCTUATIONS = r\"\"\"!()-[]{};:'\"\\,<>./?@#$%^&*_~\"\"\"\n",
    "\n",
    "STOPWORDS = [\n",
    "    \"the\", \"is\", \"are\", \"an\", \"be\", \"I\",\"an\",\"of\",\"example\",\n",
    "    \"both\",\"yeah\",\"in\", \"on\", \"at\", \"and\", \"because\", \"but\", \n",
    "    \"been\", \"by\", \"a\", \"for\",  \"from\",\n",
    "    \"all\", \"as\", \"can\", \"can't\", \"about\", \"do\", \"even\", \"every\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, stopwords: List = STOPWORDS, clean_token = [], corpus = []) -> List[str]:\n",
    "    \n",
    "    # remove the links and the user name \n",
    "    text1 = re.sub('(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])', '', text) #replacce the link with nothing, remove it\n",
    "    #find out the tags (regex might be @sdjfhkfhda with the space after), and remove it\n",
    "    text2 = re.sub('^@?(\\w){1,15}$', '', text1) #replace the username/tags to '' kinda remove it\n",
    "    text3 = re.sub('/[^a-zA-Z ]/g', ' ', text2)\n",
    "    \n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    tokens = pattern.findall(text3.lower())\n",
    "\n",
    "    #remove the stopwords\n",
    "    for token in tokens:\n",
    "        # remove the stopwords\n",
    "        if token not in stopwords:\n",
    "            clean_token.append(token)\n",
    "\n",
    "        # create a list of words\n",
    "        if token not in corpus:\n",
    "            corpus.append(token)   \n",
    "        \n",
    "        # sort the list\n",
    "        clean_token.sort()\n",
    "        corpus.sort()\n",
    "        return clean_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "corpus = [] # is the dictionary (like the things are not repeatable)\n",
    "\n",
    "for line in df:\n",
    "    tokenize(line, stopwords=STOPWORDS, clean_token=tokens, corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# cv = CountVectorizer()\n",
    "# cv_mat = cv.fit_transform(df)\n",
    "# cv_mat.todense() # change into dense\n",
    "\n",
    "# # Create word matrix\n",
    "# df_cv_words = pd.DataFrame(cv_mat.todense(),columns=cv.get_feature_names_out())\n",
    "# df_cv_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      KANYE 2024 https://t.co/Zm2pKcn12t\n",
       "1                      I VOTED üá∫üá∏ https://t.co/hlgIJUST4x\n",
       "2                    KANYE2020 üá∫üá∏ https://t.co/3kd8vrrHZQ\n",
       "3                               üïä https://t.co/tFqpKyQzkY\n",
       "4       The first vote of my life         We are here ...\n",
       "                              ...                        \n",
       "1662    try to avoid any contractual situation where y...\n",
       "1663    You have to protect your ability to create at ...\n",
       "1664    As a creative your ideas are your strongest fo...\n",
       "1665    often people working with the existing conscio...\n",
       "1666    Some people have to work within the existing c...\n",
       "Name: tweet, Length: 1667, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count vectorize\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_vocab(corpus):\n",
    "    vocab = {}\n",
    "    for i, word in enumerate(sorted(list(corpus))):\n",
    "        vocab[word] = i    \n",
    "    return vocab\n",
    "\n",
    "# create the vocab like yeah\n",
    "vocab = create_vocab(corpus=corpus)\n",
    "row, col, val = [], [], []\n",
    "\n",
    "# main loop to form the matrix\n",
    "for idx, sentence in enumerate(df):\n",
    "\n",
    "    # need to fix this part since some parts they might not recognized\n",
    "    count_word = dict(Counter(tokenize(sentence.lower())))\n",
    "\n",
    "    for word, count in count_word.items():\n",
    "        if len(word) > 2:\n",
    "            col_index = vocab.get(word)\n",
    "            if col_index >= 0:\n",
    "                row.append(idx)\n",
    "                col.append(col_index)\n",
    "                val.append(count)\n",
    "\n",
    "output_mat = csr_matrix((val, (row, col)), shape=(len(df), len(vocab))).toarray()\n",
    "output_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'11am': 1,\n",
       " '13th': 1,\n",
       " '15th': 1,\n",
       " '16th': 1,\n",
       " '18th': 1,\n",
       " '1st': 1,\n",
       " '2000s': 1,\n",
       " '2020vision': 1,\n",
       " '2020visuon': 1,\n",
       " '20s': 1,\n",
       " '21st': 1,\n",
       " '22nd': 1,\n",
       " '25th': 1,\n",
       " '2chainz': 1,\n",
       " '2chnz': 1,\n",
       " '2gthr': 1,\n",
       " '2mrw': 1,\n",
       " '2nd': 1,\n",
       " '2pm': 1,\n",
       " '350s': 1,\n",
       " '40th': 1,\n",
       " '4th': 1,\n",
       " '60s': 1,\n",
       " '700s': 1,\n",
       " '7pm': 1,\n",
       " '7th': 1,\n",
       " '8pm': 1,\n",
       " '8th': 1,\n",
       " '90s': 1,\n",
       " 'a': 1,\n",
       " 'aaaaaaaaa': 1,\n",
       " 'aaaaand': 1,\n",
       " 'aaaand': 1,\n",
       " 'abfalecbaldwin': 1,\n",
       " 'ability': 1,\n",
       " 'able': 1,\n",
       " 'abolish': 1,\n",
       " 'aborted': 1,\n",
       " 'aborting': 1,\n",
       " 'about': 1,\n",
       " 'absence': 1,\n",
       " 'absolutely': 1,\n",
       " 'ac': 1,\n",
       " 'academy': 1,\n",
       " 'accept': 1,\n",
       " 'acceptable': 1,\n",
       " 'access': 1,\n",
       " 'accessories': 1,\n",
       " 'accomplished': 1,\n",
       " 'accountable': 1,\n",
       " 'achievement': 1,\n",
       " 'acknowledge': 1,\n",
       " 'acknowledging': 1,\n",
       " 'acres': 1,\n",
       " 'across': 1,\n",
       " 'act': 1,\n",
       " 'acting': 1,\n",
       " 'actions': 1,\n",
       " 'activity': 1,\n",
       " 'actors': 1,\n",
       " 'acts': 1,\n",
       " 'actual': 1,\n",
       " 'actually': 1,\n",
       " 'ad': 1,\n",
       " 'added': 1,\n",
       " 'addicted': 1,\n",
       " 'addictive': 1,\n",
       " 'adidas': 1,\n",
       " 'admiration': 1,\n",
       " 'advanced': 1,\n",
       " 'advances': 1,\n",
       " 'advantage': 1,\n",
       " 'adverse': 1,\n",
       " 'advice': 1,\n",
       " 'affected': 1,\n",
       " 'afford': 1,\n",
       " 'affordable': 1,\n",
       " 'afraid': 1,\n",
       " 'africa': 1,\n",
       " 'african': 1,\n",
       " 'after': 1,\n",
       " 'afterwards': 1,\n",
       " 'again': 1,\n",
       " 'against': 1,\n",
       " 'age': 1,\n",
       " 'ago': 1,\n",
       " 'agree': 1,\n",
       " 'agreed': 1,\n",
       " 'agreeing': 1,\n",
       " 'aha': 1,\n",
       " 'ahead': 1,\n",
       " 'aid': 1,\n",
       " 'ain': 1,\n",
       " 'air': 1,\n",
       " 'airbnb': 1,\n",
       " 'aka': 1,\n",
       " 'akira': 1,\n",
       " 'albm': 1,\n",
       " 'album': 1,\n",
       " 'albums': 1,\n",
       " 'alcohol': 1,\n",
       " 'alec': 1,\n",
       " 'alexander': 1,\n",
       " 'alien': 1,\n",
       " 'alive': 1,\n",
       " 'all': 1,\n",
       " 'allow': 1,\n",
       " 'allowed': 1,\n",
       " 'ally': 1,\n",
       " 'almost': 1,\n",
       " 'alone': 1,\n",
       " 'alpaca': 1,\n",
       " 'alpha': 1,\n",
       " 'already': 1,\n",
       " 'alright': 1,\n",
       " 'also': 1,\n",
       " 'always': 1,\n",
       " 'am': 1,\n",
       " 'aman': 1,\n",
       " 'amazing': 1,\n",
       " 'ambitions': 1,\n",
       " 'ameer': 1,\n",
       " 'amen': 1,\n",
       " 'amend': 1,\n",
       " 'amendment': 1,\n",
       " 'amer': 1,\n",
       " 'america': 1,\n",
       " 'american': 1,\n",
       " 'americans': 1,\n",
       " 'amid': 1,\n",
       " 'amongst': 1,\n",
       " 'amount': 1,\n",
       " 'amp': 1,\n",
       " 'amphitheater': 1,\n",
       " 'amphitheatre': 1,\n",
       " 'amusing': 1,\n",
       " 'an': 1,\n",
       " 'analogy': 1,\n",
       " 'and': 1,\n",
       " 'anderson': 1,\n",
       " 'ando': 1,\n",
       " 'ands': 1,\n",
       " 'angel': 1,\n",
       " 'angeles': 1,\n",
       " 'angels': 1,\n",
       " 'animated': 1,\n",
       " 'animation': 1,\n",
       " 'anime': 1,\n",
       " 'announce': 1,\n",
       " 'another': 1,\n",
       " 'answer': 1,\n",
       " 'answers': 1,\n",
       " 'ant': 1,\n",
       " 'anthony': 1,\n",
       " 'anxiety': 1,\n",
       " 'anxious': 1,\n",
       " 'any': 1,\n",
       " 'anybody': 1,\n",
       " 'anymore': 1,\n",
       " 'anyone': 1,\n",
       " 'anything': 1,\n",
       " 'apologies': 1,\n",
       " 'apologize': 1,\n",
       " 'apologizing': 1,\n",
       " 'apology': 1,\n",
       " 'app': 1,\n",
       " 'apparel': 1,\n",
       " 'apparently': 1,\n",
       " 'appearance': 1,\n",
       " 'apple': 1,\n",
       " 'apply': 1,\n",
       " 'appreciate': 1,\n",
       " 'appreciation': 1,\n",
       " 'approach': 1,\n",
       " 'appropriate': 1,\n",
       " 'approval': 1,\n",
       " 'arc': 1,\n",
       " 'architects': 1,\n",
       " 'architecture': 1,\n",
       " 'are': 1,\n",
       " \"aren't\": 1,\n",
       " 'arena': 1,\n",
       " 'argue': 1,\n",
       " 'argument': 1,\n",
       " 'ariana': 1,\n",
       " 'arkansas': 1,\n",
       " 'arm': 1,\n",
       " 'armenia': 1,\n",
       " 'armitage': 1,\n",
       " 'arms': 1,\n",
       " 'army': 1,\n",
       " 'arnaud': 1,\n",
       " 'around': 1,\n",
       " 'art': 1,\n",
       " 'arthur': 1,\n",
       " 'artist': 1,\n",
       " 'artists': 1,\n",
       " 'arts': 1,\n",
       " 'artwork': 1,\n",
       " 'as': 1,\n",
       " 'asap': 1,\n",
       " 'ashley': 1,\n",
       " 'ask': 1,\n",
       " 'asked': 1,\n",
       " 'asking': 1,\n",
       " 'asset': 1,\n",
       " 'assimilate': 1,\n",
       " 'assist': 1,\n",
       " 'association': 1,\n",
       " 'asylum': 1,\n",
       " 'at': 1,\n",
       " 'atlanta': 1,\n",
       " 'attaching': 1,\n",
       " 'attacked': 1,\n",
       " 'audio': 1,\n",
       " 'aura': 1,\n",
       " 'authenticity': 1,\n",
       " 'authority': 1,\n",
       " 'available': 1,\n",
       " 'avenge': 1,\n",
       " 'avoid': 1,\n",
       " 'awards': 1,\n",
       " 'away': 1,\n",
       " 'awesome': 1,\n",
       " 'awhile': 1,\n",
       " 'axel': 1,\n",
       " 'azerbaijan': 1,\n",
       " 'azoff': 1,\n",
       " 'b': 1,\n",
       " 'babe': 1,\n",
       " 'babies': 1,\n",
       " 'baby': 1,\n",
       " 'back': 1,\n",
       " 'backs': 1,\n",
       " 'backstreet': 1,\n",
       " 'bad': 1,\n",
       " 'bag': 1,\n",
       " 'bags': 1,\n",
       " 'bait': 1,\n",
       " 'balance': 1,\n",
       " 'balances': 1,\n",
       " 'baldwin': 1,\n",
       " 'ball': 1,\n",
       " 'ballot': 1,\n",
       " 'ballots': 1,\n",
       " 'bank': 1,\n",
       " 'bankruptcies': 1,\n",
       " 'banton': 1,\n",
       " 'bar': 1,\n",
       " 'barely': 1,\n",
       " 'bari': 1,\n",
       " 'baring': 1,\n",
       " 'bars': 1,\n",
       " 'bart': 1,\n",
       " 'base': 1,\n",
       " 'based': 1,\n",
       " 'basel': 1,\n",
       " 'bash': 1,\n",
       " 'basic': 1,\n",
       " 'basically': 1,\n",
       " 'basketball': 1,\n",
       " 'battle': 1,\n",
       " 'bayfront': 1,\n",
       " 'bday': 1,\n",
       " 'be': 1,\n",
       " 'bearing': 1,\n",
       " 'beat': 1,\n",
       " 'beating': 1,\n",
       " 'beats': 1,\n",
       " 'beautiful': 1,\n",
       " 'beauty': 1,\n",
       " 'because': 1,\n",
       " 'become': 1,\n",
       " 'becoming': 1,\n",
       " 'bedrock': 1,\n",
       " 'bedtime': 1,\n",
       " 'bee': 1,\n",
       " 'beecroft': 1,\n",
       " 'beef': 1,\n",
       " 'been': 1,\n",
       " 'before': 1,\n",
       " 'beg': 1,\n",
       " 'begins': 1,\n",
       " 'being': 1,\n",
       " 'beings': 1,\n",
       " 'beliefs': 1,\n",
       " 'believe': 1,\n",
       " 'believing': 1,\n",
       " 'beloved': 1,\n",
       " 'below': 1,\n",
       " 'ben': 1,\n",
       " 'benz': 1,\n",
       " 'bernays': 1,\n",
       " 'best': 1,\n",
       " 'bet': 1,\n",
       " 'better': 1,\n",
       " 'between': 1,\n",
       " 'beyonc√©': 1,\n",
       " 'beyond': 1,\n",
       " 'bff': 1,\n",
       " 'bianca': 1,\n",
       " 'biden': 1,\n",
       " 'big': 1,\n",
       " 'biggest': 1,\n",
       " 'biggs': 1,\n",
       " 'bike': 1,\n",
       " 'billboard': 1,\n",
       " 'billion': 1,\n",
       " 'billionaire': 1,\n",
       " 'billions': 1,\n",
       " 'bio': 1,\n",
       " 'bipolar': 1,\n",
       " 'birthday': 1,\n",
       " 'bit': 1,\n",
       " 'black': 1,\n",
       " 'blacks': 1,\n",
       " 'bladerunner': 1,\n",
       " 'blaine': 1,\n",
       " 'blame': 1,\n",
       " 'blank': 1,\n",
       " 'blanket': 1,\n",
       " 'blasting': 1,\n",
       " 'bldng': 1,\n",
       " 'bless': 1,\n",
       " 'blessed': 1,\n",
       " 'blessings': 1,\n",
       " 'blexit': 1,\n",
       " 'blinders': 1,\n",
       " 'block': 1,\n",
       " 'blood': 1,\n",
       " 'bloody': 1,\n",
       " 'blu': 1,\n",
       " 'board': 1,\n",
       " 'boards': 1,\n",
       " 'boat': 1,\n",
       " 'bob': 1,\n",
       " 'body': 1,\n",
       " 'bolder': 1,\n",
       " 'bollerre': 1,\n",
       " 'bollore': 1,\n",
       " 'bolts': 1,\n",
       " 'bond': 1,\n",
       " 'bono': 1,\n",
       " 'book': 1,\n",
       " 'bore': 1,\n",
       " 'bored': 1,\n",
       " 'boring': 1,\n",
       " 'born': 1,\n",
       " 'boss': 1,\n",
       " 'boss777': 1,\n",
       " 'botanic': 1,\n",
       " 'both': 1,\n",
       " 'bothering': 1,\n",
       " 'bottles': 1,\n",
       " 'bout': 1,\n",
       " 'bowl': 1,\n",
       " 'boxing': 1,\n",
       " 'boy': 1,\n",
       " 'boys': 1,\n",
       " 'braids': 1,\n",
       " 'brain': 1,\n",
       " 'branch': 1,\n",
       " 'brand': 1,\n",
       " 'braun': 1,\n",
       " 'brclt': 1,\n",
       " 'break': 1,\n",
       " 'breaking': 1,\n",
       " 'breonna': 1,\n",
       " 'bridge': 1,\n",
       " 'bridgman': 1,\n",
       " 'bring': 1,\n",
       " 'bringing': 1,\n",
       " 'brn': 1,\n",
       " 'bro': 1,\n",
       " 'broccoli': 1,\n",
       " 'broke': 1,\n",
       " 'bron': 1,\n",
       " 'broooooooo': 1,\n",
       " 'brother': 1,\n",
       " 'brothers': 1,\n",
       " 'brought': 1,\n",
       " 'browns': 1,\n",
       " 'brutality': 1,\n",
       " 'bsktbl': 1,\n",
       " 'bttm': 1,\n",
       " 'bubba': 1,\n",
       " 'bubble': 1,\n",
       " 'bugs': 1,\n",
       " 'build': 1,\n",
       " 'building': 1,\n",
       " 'built': 1,\n",
       " 'buju': 1,\n",
       " 'bulk': 1,\n",
       " 'bullied': 1,\n",
       " 'bully': 1,\n",
       " 'bumped': 1,\n",
       " 'burberry': 1,\n",
       " 'burn': 1,\n",
       " 'burry': 1,\n",
       " 'business': 1,\n",
       " 'businesses': 1,\n",
       " 'but': 1,\n",
       " 'buts': 1,\n",
       " 'buuuuuuut': 1,\n",
       " 'buy': 1,\n",
       " 'buying': 1,\n",
       " 'buys': 1,\n",
       " 'by': 1,\n",
       " 'caiden': 1,\n",
       " 'calabasas': 1,\n",
       " 'calabassas': 1,\n",
       " 'calculate': 1,\n",
       " 'cali': 1,\n",
       " 'call': 1,\n",
       " 'called': 1,\n",
       " 'calling': 1,\n",
       " 'calls': 1,\n",
       " 'came': 1,\n",
       " 'camera': 1,\n",
       " 'camp': 1,\n",
       " 'campaign': 1,\n",
       " 'campus': 1,\n",
       " 'can': 1,\n",
       " \"can't\": 1,\n",
       " 'cancel': 1,\n",
       " 'canceled': 1,\n",
       " 'cancelled': 1,\n",
       " 'cancer': 1,\n",
       " 'candace': 1,\n",
       " 'candidate': 1,\n",
       " 'cannon': 1,\n",
       " 'cannot': 1,\n",
       " 'canteens': 1,\n",
       " 'cap': 1,\n",
       " 'capability': 1,\n",
       " 'capital': 1,\n",
       " 'capitalism': 1,\n",
       " 'capitalist': 1,\n",
       " 'capitalized': 1,\n",
       " 'captain': 1,\n",
       " 'car': 1,\n",
       " 'cardi': 1,\n",
       " 'care': 1,\n",
       " 'career': 1,\n",
       " 'carolina': 1,\n",
       " 'cars': 1,\n",
       " 'cartier': 1,\n",
       " 'cartoon': 1,\n",
       " 'carve': 1,\n",
       " 'cash': 1,\n",
       " 'cat': 1,\n",
       " 'catalog': 1,\n",
       " 'cathy': 1,\n",
       " 'caught': 1,\n",
       " 'cause': 1,\n",
       " 'caused': 1,\n",
       " 'causes': 1,\n",
       " 'cavs': 1,\n",
       " 'celebrate': 1,\n",
       " 'celebrating': 1,\n",
       " 'cell': 1,\n",
       " 'cent': 1,\n",
       " 'century': 1,\n",
       " 'ceo': 1,\n",
       " 'certain': 1,\n",
       " 'certainty': 1,\n",
       " 'cfo': 1,\n",
       " 'chaaaaaarged': 1,\n",
       " 'chain': 1,\n",
       " 'chains': 1,\n",
       " 'chair': 1,\n",
       " 'challenge': 1,\n",
       " 'challenged': 1,\n",
       " 'championship': 1,\n",
       " 'chance': 1,\n",
       " 'change': 1,\n",
       " 'changed': 1,\n",
       " 'changes': 1,\n",
       " 'changing': 1,\n",
       " 'channel': 1,\n",
       " 'chapelle': 1,\n",
       " 'chappelle': 1,\n",
       " 'character': 1,\n",
       " 'charge': 1,\n",
       " 'charged': 1,\n",
       " 'charlamagne': 1,\n",
       " 'chasing': 1,\n",
       " 'che': 1,\n",
       " 'check': 1,\n",
       " 'checks': 1,\n",
       " 'cher': 1,\n",
       " 'chess': 1,\n",
       " 'chi': 1,\n",
       " 'chicago': 1,\n",
       " 'chick': 1,\n",
       " 'child': 1,\n",
       " 'childhood': 1,\n",
       " 'childish': 1,\n",
       " 'children': 1,\n",
       " 'chill': 1,\n",
       " 'china': 1,\n",
       " 'chinese': 1,\n",
       " 'chld': 1,\n",
       " 'chnz': 1,\n",
       " 'chooooooooice': 1,\n",
       " 'choose': 1,\n",
       " 'chopping': 1,\n",
       " 'chose': 1,\n",
       " 'chosen': 1,\n",
       " 'christ': 1,\n",
       " 'christian': 1,\n",
       " 'christians': 1,\n",
       " 'christmas': 1,\n",
       " 'chrstn': 1,\n",
       " 'chunk': 1,\n",
       " 'cigarettes': 1,\n",
       " 'circle': 1,\n",
       " 'city': 1,\n",
       " 'civil': 1,\n",
       " 'civility': 1,\n",
       " 'clarify': 1,\n",
       " 'clarity': 1,\n",
       " 'clashes': 1,\n",
       " 'class': 1,\n",
       " 'classes': 1,\n",
       " 'classism': 1,\n",
       " 'claud': 1,\n",
       " 'claude': 1,\n",
       " 'claudio': 1,\n",
       " 'clean': 1,\n",
       " 'cleaned': 1,\n",
       " 'clear': 1,\n",
       " 'cleared': 1,\n",
       " 'clemons': 1,\n",
       " 'clf': 1,\n",
       " 'click': 1,\n",
       " 'client': 1,\n",
       " 'cline': 1,\n",
       " 'close': 1,\n",
       " 'closer': 1,\n",
       " 'closure': 1,\n",
       " 'clothes': 1,\n",
       " 'clothing': 1,\n",
       " 'clout': 1,\n",
       " 'clthng': 1,\n",
       " 'cmng': 1,\n",
       " 'co': 1,\n",
       " 'coachella': 1,\n",
       " 'coaches': 1,\n",
       " 'cobra': 1,\n",
       " 'code': 1,\n",
       " 'codes': 1,\n",
       " 'cody': 1,\n",
       " 'cole': 1,\n",
       " 'colin': 1,\n",
       " 'collaborate': 1,\n",
       " 'collaboration': 1,\n",
       " 'collection': 1,\n",
       " 'color': 1,\n",
       " 'colorado': 1,\n",
       " 'colored': 1,\n",
       " 'colors': 1,\n",
       " 'colossians': 1,\n",
       " 'combinator': 1,\n",
       " 'come': 1,\n",
       " 'comedy': 1,\n",
       " 'comfortable': 1,\n",
       " 'coming': 1,\n",
       " 'command': 1,\n",
       " 'commentary': 1,\n",
       " 'comments': 1,\n",
       " 'commercial': 1,\n",
       " 'committing': 1,\n",
       " 'common': 1,\n",
       " 'communicate': 1,\n",
       " 'communication': 1,\n",
       " 'communications': 1,\n",
       " 'community': 1,\n",
       " 'companies': 1,\n",
       " 'company': 1,\n",
       " 'compassion': 1,\n",
       " 'compassionate': 1,\n",
       " 'competition': 1,\n",
       " 'complaint': 1,\n",
       " 'complete': 1,\n",
       " 'completely': 1,\n",
       " 'complicated': 1,\n",
       " 'concept': 1,\n",
       " 'concerned': 1,\n",
       " 'concerning': 1,\n",
       " 'concert': 1,\n",
       " 'concerts': 1,\n",
       " 'condemned': 1,\n",
       " 'condescension': 1,\n",
       " 'condition': 1,\n",
       " 'conditioned': 1,\n",
       " 'conditions': 1,\n",
       " 'condo': 1,\n",
       " 'condolences': 1,\n",
       " 'confidence': 1,\n",
       " 'conflict': 1,\n",
       " 'confused': 1,\n",
       " 'confusion': 1,\n",
       " 'congrats': 1,\n",
       " 'congratulations': 1,\n",
       " 'connect': 1,\n",
       " 'connected': 1,\n",
       " 'connection': 1,\n",
       " 'cons': 1,\n",
       " 'conscience': 1,\n",
       " 'conscious': 1,\n",
       " 'consciousness': 1,\n",
       " 'consensus': 1,\n",
       " 'consequences': 1,\n",
       " 'conservatives': 1,\n",
       " 'consider': 1,\n",
       " 'consideration': 1,\n",
       " 'constant': 1,\n",
       " 'constantly': 1,\n",
       " 'construct': 1,\n",
       " 'consumerism': 1,\n",
       " 'contact': 1,\n",
       " 'contagious': 1,\n",
       " 'content': 1,\n",
       " 'context': 1,\n",
       " 'continues': 1,\n",
       " 'contract': 1,\n",
       " 'contracts': 1,\n",
       " 'contractual': 1,\n",
       " 'contribute': 1,\n",
       " 'control': 1,\n",
       " 'controlled': 1,\n",
       " 'controlling': 1,\n",
       " 'controls': 1,\n",
       " 'controversy': 1,\n",
       " 'conventional': 1,\n",
       " 'conversation': 1,\n",
       " 'conversations': 1,\n",
       " 'convo': 1,\n",
       " 'cook': 1,\n",
       " 'cool': 1,\n",
       " 'cooler': 1,\n",
       " 'cop': 1,\n",
       " 'copyright': 1,\n",
       " 'core': 1,\n",
       " 'correct': 1,\n",
       " 'correction': 1,\n",
       " 'cortisone': 1,\n",
       " 'cost': 1,\n",
       " 'costs': 1,\n",
       " 'costume': 1,\n",
       " 'couch': 1,\n",
       " 'could': 1,\n",
       " \"could've\": 1,\n",
       " 'counted': 1,\n",
       " 'countless': 1,\n",
       " 'countries': 1,\n",
       " 'country': 1,\n",
       " 'couple': 1,\n",
       " 'course': 1,\n",
       " 'cover': 1,\n",
       " 'coverage': 1,\n",
       " 'covered': 1,\n",
       " 'covid': 1,\n",
       " 'craft': 1,\n",
       " 'crafting': 1,\n",
       " 'crash': 1,\n",
       " 'crater': 1,\n",
       " 'crayon': 1,\n",
       " 'craziest': 1,\n",
       " 'crazy': 1,\n",
       " 'create': 1,\n",
       " 'created': 1,\n",
       " 'creates': 1,\n",
       " 'creating': 1,\n",
       " 'creative': 1,\n",
       " 'creatives': 1,\n",
       " 'creator': 1,\n",
       " 'credit': 1,\n",
       " 'creed': 1,\n",
       " 'crenshaw': 1,\n",
       " 'crew': 1,\n",
       " 'crib': 1,\n",
       " 'cried': 1,\n",
       " 'cries': 1,\n",
       " 'crimes': 1,\n",
       " 'critical': 1,\n",
       " 'croc': 1,\n",
       " 'crooked': 1,\n",
       " 'cross': 1,\n",
       " 'crossed': 1,\n",
       " 'crowds': 1,\n",
       " 'cry': 1,\n",
       " 'crying': 1,\n",
       " 'crystal': 1,\n",
       " 'cthagod': 1,\n",
       " 'cube': 1,\n",
       " 'cudi': 1,\n",
       " 'cult': 1,\n",
       " 'culturally': 1,\n",
       " 'culture': 1,\n",
       " 'curator': 1,\n",
       " 'cure': 1,\n",
       " 'currency': 1,\n",
       " 'current': 1,\n",
       " 'currently': 1,\n",
       " 'cut': 1,\n",
       " 'cvr': 1,\n",
       " 'cyhi': 1,\n",
       " 'd': 1,\n",
       " 'da': 1,\n",
       " 'dababy': 1,\n",
       " 'dad': 1,\n",
       " 'daddy': 1,\n",
       " 'daily': 1,\n",
       " 'damage': 1,\n",
       " 'dan': 1,\n",
       " 'dance': 1,\n",
       " 'dancers': 1,\n",
       " 'dances': 1,\n",
       " 'dangerous': 1,\n",
       " 'daniel': 1,\n",
       " 'dao': 1,\n",
       " 'dark': 1,\n",
       " 'date': 1,\n",
       " 'dated': 1,\n",
       " 'dates': 1,\n",
       " 'daughter': 1,\n",
       " 'daughters': 1,\n",
       " 'dave': 1,\n",
       " 'david': 1,\n",
       " 'davidwebb': 1,\n",
       " 'day': 1,\n",
       " 'days': 1,\n",
       " 'daytona': 1,\n",
       " 'dd': 1,\n",
       " 'de': 1,\n",
       " 'dead': 1,\n",
       " 'deadliest': 1,\n",
       " 'deadpool': 1,\n",
       " 'deal': 1,\n",
       " 'dealing': 1,\n",
       " 'deals': 1,\n",
       " 'dear': 1,\n",
       " 'dearly': 1,\n",
       " 'death': 1,\n",
       " 'deaths': 1,\n",
       " 'debt': 1,\n",
       " 'decacorn': 1,\n",
       " 'decade': 1,\n",
       " 'decided': 1,\n",
       " 'decisions': 1,\n",
       " 'dedicate': 1,\n",
       " 'dedicated': 1,\n",
       " 'deeply': 1,\n",
       " 'defined': 1,\n",
       " 'definitely': 1,\n",
       " 'deleted': 1,\n",
       " 'delgato': 1,\n",
       " 'deliver': 1,\n",
       " 'delivered': 1,\n",
       " 'demanding': 1,\n",
       " 'dementional': 1,\n",
       " 'democracy': 1,\n",
       " 'democrat': 1,\n",
       " 'democratic': 1,\n",
       " 'democrats': 1,\n",
       " 'demonization': 1,\n",
       " 'demonize': 1,\n",
       " 'demonized': 1,\n",
       " 'denis': 1,\n",
       " 'dennis': 1,\n",
       " 'department': 1,\n",
       " 'dependants': 1,\n",
       " 'dependent': 1,\n",
       " 'dependents': 1,\n",
       " 'desalination': 1,\n",
       " 'describe': 1,\n",
       " 'described': 1,\n",
       " 'description': 1,\n",
       " 'desert': 1,\n",
       " 'deserve': 1,\n",
       " 'design': 1,\n",
       " 'designed': 1,\n",
       " 'designer': 1,\n",
       " 'designers': 1,\n",
       " 'designing': 1,\n",
       " 'designs': 1,\n",
       " 'desire': 1,\n",
       " 'destigmitizing': 1,\n",
       " 'destiny': 1,\n",
       " 'destroy': 1,\n",
       " 'detroit': 1,\n",
       " 'detroitbruce': 1,\n",
       " 'developing': 1,\n",
       " 'development': 1,\n",
       " 'develops': 1,\n",
       " 'devil': 1,\n",
       " 'dexamethasone': 1,\n",
       " 'dexter': 1,\n",
       " 'dezer': 1,\n",
       " 'diagnosed': 1,\n",
       " 'dialogue': 1,\n",
       " 'diamonds': 1,\n",
       " 'diatribe': 1,\n",
       " 'dick': 1,\n",
       " 'dictionary': 1,\n",
       " 'did': 1,\n",
       " 'didn': 1,\n",
       " \"didn't\": 1,\n",
       " 'die': 1,\n",
       " 'died': 1,\n",
       " 'diet': 1,\n",
       " 'different': 1,\n",
       " 'difficult': 1,\n",
       " 'digging': 1,\n",
       " 'dinner': 1,\n",
       " 'dior': 1,\n",
       " 'directed': 1,\n",
       " 'directly': 1,\n",
       " 'director': 1,\n",
       " 'disagree': 1,\n",
       " 'disconnect': 1,\n",
       " 'discredit': 1,\n",
       " 'discussed': 1,\n",
       " 'discussing': 1,\n",
       " 'discussions': 1,\n",
       " 'disease': 1,\n",
       " 'disgruntled': 1,\n",
       " 'disguise': 1,\n",
       " 'dishes': 1,\n",
       " 'dismiss': 1,\n",
       " 'disney': 1,\n",
       " 'disorder': 1,\n",
       " 'display': 1,\n",
       " 'disrespectful': 1,\n",
       " 'diss': 1,\n",
       " 'disses': 1,\n",
       " 'dissing': 1,\n",
       " 'disssing': 1,\n",
       " 'distancing': 1,\n",
       " 'distract': 1,\n",
       " 'distraction': 1,\n",
       " 'distractive': 1,\n",
       " 'distribution': 1,\n",
       " 'disturbed': 1,\n",
       " 'divine': 1,\n",
       " 'divisions': 1,\n",
       " 'dnd': 1,\n",
       " 'do': 1,\n",
       " 'doc': 1,\n",
       " 'doctor': 1,\n",
       " 'documentary': 1,\n",
       " 'does': 1,\n",
       " 'doesn': 1,\n",
       " 'doing': 1,\n",
       " 'dojo': 1,\n",
       " 'dojos': 1,\n",
       " 'dollar': 1,\n",
       " 'dollars': 1,\n",
       " 'dome': 1,\n",
       " 'domes': 1,\n",
       " 'don': 1,\n",
       " \"don't\": 1,\n",
       " 'donald': 1,\n",
       " 'donda': 1,\n",
       " 'done': 1,\n",
       " 'dont': 1,\n",
       " 'door': 1,\n",
       " 'doors': 1,\n",
       " 'dopamine': 1,\n",
       " 'dorsey': 1,\n",
       " 'doubt': 1,\n",
       " 'doubters': 1,\n",
       " 'doveawards': 1,\n",
       " 'down': 1,\n",
       " 'download': 1,\n",
       " 'dr': 1,\n",
       " 'dragon': 1,\n",
       " 'drake': 1,\n",
       " 'drakes': 1,\n",
       " 'dre': 1,\n",
       " 'dream': 1,\n",
       " 'dreamed': 1,\n",
       " 'dreams': 1,\n",
       " 'drinking': 1,\n",
       " 'drip': 1,\n",
       " 'drive': 1,\n",
       " 'driven': 1,\n",
       " 'driving': 1,\n",
       " 'drone': 1,\n",
       " 'drop': 1,\n",
       " 'dropbox': 1,\n",
       " 'drops': 1,\n",
       " 'drown': 1,\n",
       " 'drug': 1,\n",
       " 'dsgnd': 1,\n",
       " 'due': 1,\n",
       " 'dues': 1,\n",
       " 'duper': 1,\n",
       " 'during': 1,\n",
       " 'dvr': 1,\n",
       " 'dying': 1,\n",
       " 'dylan': 1,\n",
       " 'dynamics': 1,\n",
       " 'd≈çj≈ç': 1,\n",
       " 'e': 1,\n",
       " 'each': 1,\n",
       " 'ear': 1,\n",
       " 'earlier': 1,\n",
       " 'early': 1,\n",
       " 'earners': 1,\n",
       " 'earth': 1,\n",
       " 'earthly': 1,\n",
       " 'easier': 1,\n",
       " 'east': 1,\n",
       " 'easy': 1,\n",
       " 'eat': 1,\n",
       " 'eating': 1,\n",
       " 'ebro': 1,\n",
       " 'eco': 1,\n",
       " 'economic': 1,\n",
       " 'economically': 1,\n",
       " 'ecrou': 1,\n",
       " 'ed': 1,\n",
       " 'eddie': 1,\n",
       " 'edison': 1,\n",
       " 'edit': 1,\n",
       " 'edt': 1,\n",
       " 'education': 1,\n",
       " 'edward': 1,\n",
       " 'effect': 1,\n",
       " 'ego': 1,\n",
       " 'egos': 1,\n",
       " 'eight': 1,\n",
       " 'either': 1,\n",
       " 'elder': 1,\n",
       " 'electric': 1,\n",
       " 'eli': 1,\n",
       " 'elitism': 1,\n",
       " 'elon': 1,\n",
       " 'else': 1,\n",
       " 'embarrassingly': 1,\n",
       " 'embrace': 1,\n",
       " 'embraced': 1,\n",
       " 'eminem': 1,\n",
       " 'emoji': 1,\n",
       " 'emojis': 1,\n",
       " 'emory': 1,\n",
       " 'emotion': 1,\n",
       " 'emotional': 1,\n",
       " 'empathetic': 1,\n",
       " 'empathy': 1,\n",
       " 'emphasis': 1,\n",
       " 'employees': 1,\n",
       " 'empowered': 1,\n",
       " 'empty': 1,\n",
       " 'encouragement': 1,\n",
       " 'end': 1,\n",
       " 'ended': 1,\n",
       " 'ending': 1,\n",
       " 'endless': 1,\n",
       " 'endpolicebrutalityinnigeria': 1,\n",
       " 'ends': 1,\n",
       " 'enemy': 1,\n",
       " 'energized': 1,\n",
       " 'energy': 1,\n",
       " 'enforced': 1,\n",
       " 'engaged': 1,\n",
       " 'english': 1,\n",
       " 'enhance': 1,\n",
       " 'enjoy': 1,\n",
       " 'enough': 1,\n",
       " 'enslaved': 1,\n",
       " 'entertaining': 1,\n",
       " 'entertainment': 1,\n",
       " 'entire': 1,\n",
       " 'entrepreneur': 1,\n",
       " 'environment': 1,\n",
       " 'envision': 1,\n",
       " 'ephesians': 1,\n",
       " 'episode': 1,\n",
       " 'equity': 1,\n",
       " 'eration': 1,\n",
       " 'erik': 1,\n",
       " 'ernest': 1,\n",
       " 'ernesto': 1,\n",
       " 'erratic': 1,\n",
       " 'escape': 1,\n",
       " 'especially': 1,\n",
       " 'espn': 1,\n",
       " 'esskeetit': 1,\n",
       " 'established': 1,\n",
       " 'etc': 1,\n",
       " 'eternal': 1,\n",
       " 'etiquette': 1,\n",
       " 'europe': 1,\n",
       " 'evan': 1,\n",
       " 'even': 1,\n",
       " 'ever': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_counter(corpus):\n",
    "    return dict(Counter(corpus)) \n",
    "\n",
    "word_counter(corpus=corpus)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
